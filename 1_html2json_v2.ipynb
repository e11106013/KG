{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錯誤-檔案找不到題目:水源永續：多功能石門水庫的興建(204).html\n",
      "錯誤-文章找不到作者:水源永續：多功能石門水庫的興建(204).html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def word_filter(words):      \n",
    "   words = words.replace(\"\\n\", \"\")\n",
    "   words = words.replace(\"\\r\", \"\")\n",
    "   words = words.replace(\"&nbsp;\", \"\")\n",
    "   words = words.replace(\"\t\", \"\")\n",
    "   words = words.replace(\" \", \"\")\n",
    "   words = words.strip()\n",
    "   #s.replace('&nbsp;',' ')\n",
    "   #article = re.sub(r'\t', '', article)\n",
    "   #article = re.sub(r' ', '', article)\n",
    "   return words\n",
    "\n",
    "def remove_subtitle(name):      \n",
    "   name = name.replace(\".html\", \"\")\n",
    "   name = name.replace(\".pdf\", \"\")\n",
    "   name = name.replace(\".docx\", \"\")\n",
    "   return name\n",
    "\n",
    "# 轉換全部HTML成為JSON格式\n",
    "\n",
    "# Handle files in the directory\n",
    "html_dir = \"./docs/input/html/\"\n",
    "json_dir = \"./docs/output/0_json/\"\n",
    "paths = os.listdir('./docs/input/html/')\n",
    "\n",
    "\n",
    "for files in paths:\n",
    "    files_id = [] \n",
    "    titles_id = [] \n",
    "    intro_id = []\n",
    "    author_id = []\n",
    "    articles_id = []\n",
    "    ref_id=[]\n",
    "\n",
    "    try:\n",
    "        with open(html_dir + files, encoding=\"utf-8\") as f:            \n",
    "            #print('\\r\\n'+ '檔案名稱:' + files)              \n",
    "            files_id.append(files)\n",
    "\n",
    "            soup = BeautifulSoup(f, \"html.parser\")                                    \n",
    "            # 尋找以文件起點<div class=\"alohascontent\" id=\"ctl00_divContent\">   \n",
    "            all_content = soup.find(\"div\", class_ = \"alohascontent\")             \n",
    "            # print (all_content)  # 列印網頁內容，包含html tag \n",
    "      \n",
    "            # 文章題目                                \n",
    "            try:\n",
    "                title = all_content.find(\"p\", class_ = \"alohastitle\").text   #找文章的題目\n",
    "                #print('題目:' + files + word_filter(title))\n",
    "                titles_id.append(word_filter(title)) \n",
    "            except:\n",
    "                try:   \n",
    "                    title = soup.find(\"div\", class_ = \"info-title\").text   #找文章的題目\n",
    "                    #print('The above code is a comment written in Chinese characters. It appears to be a placeholder for a question or topic.\n",
    "                    題目:' + files + word_filter(title))\n",
    "                    titles_id.append(word_filter(title)) \n",
    "                except:\n",
    "                    print('錯誤-檔案找不到題目:' + files)\n",
    "                    titles_id.append(np.nan)     \n",
    "            \n",
    "            # 文章引言                              \n",
    "            try:\n",
    "                introduction = all_content.find(\"em\").text # 找文章引言<em>               \n",
    "                #print('引言:' + word_filter(introduction))\n",
    "                intro_id.append(word_filter(introduction))  \n",
    "            except:\n",
    "                try:\n",
    "                    introduction = all_content.find(\"p\", class_ = {\"subtitle1\",\"introduction\"}).text # 找文章引言<em>\n",
    "                    #print('引言:' + word_filter(introduction))\n",
    "                    intro_id.append(word_filter(introduction))    \n",
    "                except:\n",
    "                    try:\n",
    "                        introduction = all_content.find(\"p\", class_ = \"text-right author\").findPrevious('p').text # 找文章引言<em>\n",
    "                        #print('引言:' + word_filter(introduction))\n",
    "                        intro_id.append(word_filter(introduction))\n",
    "                    except:\n",
    "                        try:\n",
    "                            introduction = all_content.find(\"div\", class_ = \"clearfix\").findNext('p').text # 找文章引言<em>\n",
    "                            #print('引言:' + word_filter(introduction))\n",
    "                            intro_id.append(word_filter(introduction))\n",
    "                        except:\n",
    "                            try:\n",
    "                                introduction = soup.find(\"div\", class_ = \"content\").findNext(\"div\" , class_ = \"wrap\").findNext('p').text   #找文章的題目                                    \n",
    "                                #print('引言:' + word_filter(introduction))\n",
    "                                all_out.append(word_filter(introduction))\n",
    "                            except:\n",
    "                                #print('錯誤-文章找不到引言:' + files)\n",
    "                                intro_id.append(np.nan)               \n",
    "           \n",
    "            # 作者\n",
    "            try:\n",
    "                author = all_content.find(\"p\", class_ = {\"text-right author\",\"credit\"}).text # 找文章引言<em>               \n",
    "                #print('作者:' + word_filter(author))\n",
    "                author_id.append(word_filter(author))  \n",
    "            except:\n",
    "                try:\n",
    "                    author = all_content.find(\"div\", class_ = {\"subfont_12R\"}).text # 找文章引言<em>               \n",
    "                    #print('作者:' + word_filter(author))\n",
    "                    author_id.append(word_filter(author))  \n",
    "                except:\n",
    "                    try:\n",
    "                        author = all_content.find('span', attrs={\"style\": {\"color:#8e44ad;font-size:0.933em;\",\"color:#800080;font-size:0.933em;\"}}).text # 找文章引言<em>               \n",
    "                        #print('作者:\"' + word_filter(author))\n",
    "                        author_id.append(word_filter(author))  \n",
    "                    except:\n",
    "                        try:\n",
    "                            author = soup.find(\"div\", class_=\"content\").findNext(\"div\" , class_ = \"wrap\").findNext(\"div\", class_=\"text-right\").text   #找文章的題目                                    \n",
    "                            #print('作者:' + word_filter(author))\n",
    "                            author_id.append(word_filter(author)) \n",
    "                        except:\n",
    "                            print('錯誤-文章找不到作者:' + files)\n",
    "                            author_id.append(np.nan)\n",
    "\n",
    "            # 內文/參考內文:\n",
    "                  # 內文:\n",
    "\n",
    "            try:\n",
    "                for articles in all_content.find('p', attrs={\"class\": \"text-right author\"}).find_all_next(\"div\",class_={\"col-md-12\",\"col-md-6\"}):               \n",
    "                    for articles1 in articles.find_all(\"p\"):\n",
    "                        #print('內文\"1\":' + word_filter(articles1.text))\n",
    "                        articles_id.append(word_filter(articles1.text))\n",
    "                for articles in all_content.find_all_next(\"div\",class_={\"caption\",\"panel-body\"}):                \n",
    "                   #print('內文參考\"1\":' + word_filter(articles.text))  \n",
    "                   ref_id.append(word_filter(articles.text))         \n",
    "            except:\n",
    "                try:\n",
    "                    for articles in all_content.find('span', attrs={\"style\": {\"color:#8e44ad;font-size:0.933em;\"}}):\n",
    "                        if articles.find_all_next(\"p\", attrs={\"align\":\"left\"}):\n",
    "                            for articles1 in articles.find_all_next(\"p\", attrs={\"align\":\"left\"}):  \n",
    "                                #print('內文\"2\":' + word_filter(articles1.text))\n",
    "                                articles_id.append(word_filter(articles1.text))   \n",
    "                        if articles.find_all_next(lambda tag: tag.name == 'p' and not tag.attrs): \n",
    "                            for articles1 in articles.find_all_next(lambda tag: tag.name == 'p' and not tag.attrs):  \n",
    "                                #print('內文\"2.1\":' + word_filter(articles1.text))\n",
    "                                articles_id.append(word_filter(articles1.text))\n",
    "                                 \n",
    "                        if articles.find_all_next(\"td\", colspan={\"2\",\"3\"}):\n",
    "                            for articles1 in articles.find_all_next(\"td\", colspan={\"2\",\"3\"}):  \n",
    "                                #print('內文參考\"2.1\":' + word_filter(articles1.text))\n",
    "                                ref_id.append(word_filter(articles1.text))   \n",
    "                        elif articles.find_all_next(\"p\", style=\"text-align: left;\"):\n",
    "                            for articles1 in articles.find_all_next(\"p\", style=\"text-align: left;\"):  \n",
    "                                #print('內文參考\"2.2\":' + word_filter(articles1.text)) \n",
    "                                ref_id.append(word_filter(articles1.text))  \n",
    "                        elif articles.find_all_next(\"div\", style=\"text-align: left;padding-left:520px;\"):\n",
    "                            for articles1 in articles.find_all_next(\"div\", style=\"text-align: left;padding-left:520px;\"):   \n",
    "                                #print('內文參考\"2.3\":' + word_filter(articles1.text))\n",
    "                                ref_id.append(word_filter(articles1.text))    \n",
    "                        elif articles.find_all_next(\"span\", style=\"font-size:0.933em;\"):\n",
    "                            for articles1 in articles.find_all_next(\"span\", style=\"font-size:0.933em;\"):   \n",
    "                                #print('內文參考\"2.4\":' + word_filter(articles1.text))  \n",
    "                                ref_id.append(word_filter(articles1.text))  \n",
    "\n",
    "                except:\n",
    "                    try:\n",
    "                        for articles in all_content.find('span', attrs={\"style\": {\"color:#800080;font-size:0.933em;\"}}):\n",
    "                            if articles.find_all_next(lambda tag: tag.name == 'p' and not tag.attrs): \n",
    "                                for articles1 in articles.find_all_next(lambda tag: tag.name == 'p' and not tag.attrs):  \n",
    "                                    #print('內文\"3\":' + word_filter(articles1.text))\n",
    "                                    articles_id.append(word_filter(articles1.text))\n",
    "                            #elif articles.find_all(\"p\", attrs={\"align\":\"left\"}):                  \n",
    "                            if articles.find_all_next(\"span\", style=\"font-size:0.933em;\"):\n",
    "                                for articles1 in articles.find_all_next(\"span\", style=\"font-size:0.933em;\"):   \n",
    "                                    #print('內文參考\"3.1\":' + word_filter(articles1.text))  \n",
    "                                    ref_id.append(word_filter(articles1.text))                                     \n",
    "                            elif articles.find_all_next(\"div\", style=\"text-align: left;padding-left:520px;\"):\n",
    "                                for articles1 in articles.find_all_next(\"div\", style=\"text-align: left;padding-left:520px;\"):   \n",
    "                                    #print('內文參考\"3.2\":' + word_filter(articles1.text)) \n",
    "                                    ref_id.append(word_filter(articles1.text))   \n",
    "                    except:\n",
    "                        try:\n",
    "                            for articles in all_content.find('div', attrs={\"class\": \"subfont_12R\"}):\n",
    "                                for articles1 in articles.find_all_next(\"p\",attrs={\"class\":\"subfont_13\"}):          \n",
    "                                    #print('內文\"4\":' + word_filter(articles1.text))\n",
    "                                    articles_id.append(word_filter(articles1.text))\n",
    "                            for articles in all_content.find('div', attrs={\"class\": \"subfont_12R\"}):\n",
    "                                for articles1 in articles.find_all_next(\"div\",attrs={\"class\":\"subfont_12\"}):     \n",
    "                                    #print('內文參考\"4\":' + word_filter(articles1.text))\n",
    "                                    ref_id.append(word_filter(articles1.text))                 \n",
    "                        except:\n",
    "                            try:\n",
    "                                for articles in all_content.find(\"p\", attrs={\"class\": \"credit\"}):                        \n",
    "                                    for articles1 in articles.find_all_next(\"p\"):                                          \n",
    "                                        #print('內文\"5\":' + word_filter(articles1.text))\n",
    "                                        articles_id.append(word_filter(articles1.text))\n",
    "                                    for articles1 in articles.find_all_next(\"figcaption\"):     \n",
    "                                        #print('內文參考\"5\":' + word_filter(articles1.text))\n",
    "                                        ref_id.append(word_filter(articles1.text))                   \n",
    "                            except:\n",
    "                                try:\n",
    "                                    #print('內文\"5\":' + files)                                \n",
    "                                    for articles in soup.find('div', attrs={\"class\": \"content\"}).find_all_next(\"p\"):     \n",
    "                                        #print('內文\"6\":' + word_filter(articles.text))\n",
    "                                        articles_id.append(word_filter(articles.text))  \n",
    "                                    for articles in soup.find('div', attrs={\"class\": \"content\"}).find_all_next(\"div\",class_=\"caption\"):                #         print(articles.text)\n",
    "                                        #print('內文參考\"6\":' + word_filter(articles.text))\n",
    "                                        ref_id.append(word_filter(articles.text))                              \n",
    "                                except:\n",
    "                                    #print('錯誤-文章找不到內文/參考:' + files)\n",
    "                                    articles_id.append(np.nan) \n",
    "                                    ref_id.append(np.nan) \n",
    "  \n",
    "        dict = {\"filename\": files_id, \"titles\": titles_id, \"intro\": intro_id, \"author\": author_id, \"articles\": list(filter(None, articles_id)), \"reference\": list(filter(None, ref_id))}\n",
    "        with open(json_dir + remove_subtitle(files) + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(dict, indent=4, ensure_ascii=False))\n",
    "\n",
    "    except IOError as exc:\n",
    "        raise\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
