{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import DCTERMS, RDF, RDFS, XSD, FOAF, DC\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# BASE URI\n",
    "BASE_URI = \"http://example.org/\"\n",
    "EX = Namespace(BASE_URI)\n",
    "WDT = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "CC = Namespace(\"http://creativecommons.org/ns#\")\n",
    "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "dc1 = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def init_graph():\n",
    "    g = Graph()\n",
    "    g.bind(\"ex\", EX)\n",
    "    g.bind(\"wd\", WD)\n",
    "    g.bind(\"wdt\", WDT)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"cc\", CC)\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"dc\", dc1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blank_node(graph, subject):\n",
    "    # ä½¿ç”¨ Blank Node ä¾†ç”ŸæˆåŒ¿åç¯€é»\n",
    "    blank_node = URIRef(f\"_:b{hash(subject)}\")\n",
    "    return blank_node\n",
    "\n",
    "def convert_article_to_rdf(graph, article_data):\n",
    "    file_id = article_data[\"filename\"].replace(\" \", \"_\")\n",
    "    article_uri = URIRef(EX[f\"doc/{file_id}\"])\n",
    "    \n",
    "    # ä½¿ç”¨æ¨™æº–é¡å‹\n",
    "    graph.add((article_uri, RDF.type, FOAF.Document))\n",
    "\n",
    "    # æ¨™é¡Œï¼ˆä½¿ç”¨ dc:titleï¼‰\n",
    "    title = article_data.get(\"titles\", [])\n",
    "    if title:\n",
    "        graph.add((article_uri, DC.title, Literal(title[0])))\n",
    "\n",
    "    # ç¶²å€ï¼ˆä½¿ç”¨ dc:source æˆ– dc:identifierï¼‰\n",
    "    url = article_data.get(\"URL\")\n",
    "    if url:\n",
    "        graph.add((article_uri, DC.identifier, URIRef(url)))\n",
    "\n",
    "    # æ—¥æœŸï¼ˆdc:dateï¼‰\n",
    "    raw_date = article_data.get(\"published_date\")\n",
    "    if raw_date:\n",
    "        for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\"):\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(raw_date, fmt)\n",
    "                graph.add((article_uri, DC.date, Literal(parsed_date.date().isoformat(), datatype=XSD.date)))\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"âš ï¸ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼ˆè·³éï¼‰: {raw_date} in {article_data['filename']}\")\n",
    "\n",
    "    # ä½œè€…ï¼ˆdc:creatorï¼‰\n",
    "    author = article_data.get(\"author\")\n",
    "    if isinstance(author, str):\n",
    "        graph.add((article_uri, DC.creator, Literal(author)))\n",
    "    elif isinstance(author, list):\n",
    "        for a in author:\n",
    "            graph.add((article_uri, DC.creator, Literal(a)))\n",
    "\n",
    "    # Metadata ä¸­çš„æª”æ¡ˆè³‡è¨Š\n",
    "    metadata_raw = article_data.get(\"metadata\", [])\n",
    "    if isinstance(metadata_raw, dict):\n",
    "        metadata_list = [metadata_raw]\n",
    "    elif isinstance(metadata_raw, list):\n",
    "        metadata_list = [item for item in metadata_raw if isinstance(item, dict)]\n",
    "    else:\n",
    "        metadata_list = []\n",
    "\n",
    "    for metadata in metadata_list:\n",
    "        file_number = metadata.get(\"æª”è™Ÿ\")\n",
    "        if not file_number:\n",
    "            continue\n",
    "\n",
    "        file_uri = generate_blank_node(graph, f\"file_{file_number}\")\n",
    "        graph.add((file_uri, RDF.type, EX.ArchiveFile))\n",
    "        graph.add((file_uri, WDT[\"P217\"], Literal(file_number)))\n",
    "\n",
    "        # èˆ‡æ–‡ä»¶é—œè¯\n",
    "        graph.add((article_uri, EX.relatedFile, file_uri))\n",
    "\n",
    "        if \"æ¡ˆå\" in metadata:\n",
    "            graph.add((file_uri, DC.title, Literal(metadata[\"æ¡ˆå\"])))\n",
    "        if \"ä¾†æºæ©Ÿé—œ\" in metadata:\n",
    "            graph.add((file_uri, DC.creator, Literal(metadata[\"ä¾†æºæ©Ÿé—œ\"])))\n",
    "        if \"ç®¡æœ‰æ©Ÿé—œ\" in metadata:\n",
    "            graph.add((file_uri, DC.publisher, Literal(metadata[\"ç®¡æœ‰æ©Ÿé—œ\"])))\n",
    "        if \"æª”æ¡ˆå½±åƒ\" in metadata:\n",
    "            graph.add((file_uri, EX.imageNumber, Literal(metadata[\"æª”æ¡ˆå½±åƒ\"])))\n",
    "        \n",
    "        graph.add((article_uri, DCTERMS.rights, Literal(\"æœ¬è³‡æ–™ä¾æ“šå‰µç”¨ CC å§“åæ¨™ç¤º-éå•†æ¥­æ€§ 3.0 æˆæ¬Šæ¢æ¬¾é‡‹å‡º\")))\n",
    "        graph.add((article_uri, URIRef(\"http://creativecommons.org/ns#license\"), URIRef(\"http://creativecommons.org/licenses/by-nc/3.0/\")))\n",
    "        graph.add((article_uri, URIRef(\"http://creativecommons.org/ns#attributionName\"), Literal(\"æª”æ¡ˆç®¡ç†å±€\")))\n",
    "        graph.add((article_uri, URIRef(\"http://creativecommons.org/ns#attributionURL\"), URIRef(\"https://www.archives.gov.tw\")))\n",
    "\n",
    "    # è™•ç†å…¶ä»–è£œå……æ¬„ä½\n",
    "    other_fields = {\n",
    "        \"source\": DC.source,\n",
    "        \"license\": URIRef(\"http://creativecommons.org/licenses/by-nc/3.0/\"),\n",
    "        \"type\": EX.type,\n",
    "        \"theme\": EX.theme,\n",
    "        \"place\": EX.place,\n",
    "        \"time\": EX.time,\n",
    "    }\n",
    "\n",
    "    for field, predicate in other_fields.items():\n",
    "        value = article_data.get(field)\n",
    "        if value:\n",
    "            if isinstance(predicate, URIRef):\n",
    "                # å¦‚æœæ˜¯ license URIï¼Œç›´æ¥åŠ å…¥\n",
    "                graph.add((article_uri, DC.rights, predicate))\n",
    "            else:\n",
    "                if isinstance(value, list):\n",
    "                    for v in value:\n",
    "                        graph.add((article_uri, predicate, Literal(v)))\n",
    "                else:\n",
    "                    graph.add((article_uri, predicate, Literal(value)))\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 4. ä¸»ç¨‹å¼é‚è¼¯ï¼ˆé¸æ“‡å–®æª”æˆ–å¤šæª”è¼¸å‡ºï¼‰\n",
    "\n",
    "\n",
    "def convert_json_to_rdf(json_path, output_dir=\"output\", one_file=True):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if one_file:\n",
    "        g = init_graph()\n",
    "        for article in data:\n",
    "            convert_article_to_rdf(g, article)\n",
    "        g.serialize(destination=output_dir / \"all_articles.ttl\", format=\"turtle\", base=BASE_URI)\n",
    "    else:\n",
    "        for article in data:\n",
    "            g = init_graph()\n",
    "            convert_article_to_rdf(g, article)\n",
    "            filename = article[\"filename\"].replace(\" \", \"_\")\n",
    "            g.serialize(destination=output_dir / f\"{filename}.ttl\", format=\"turtle\", base=BASE_URI)\n",
    "\n",
    "\n",
    "## 5. æ‰¹æ¬¡è®€å–ç›®éŒ„ä¸­ JSON æª”æ¡ˆä¸¦æå–æ¬„ä½\n",
    "\n",
    "\n",
    "def extract_json_fields(json_dir):\n",
    "    results = []\n",
    "    for filename in os.listdir(json_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            path = os.path.join(json_dir, filename)\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                result = {\n",
    "                    \"filename\": filename,\n",
    "                    \"titles\": data.get(\"titles\", []),\n",
    "                    \"author\": data.get(\"author\", []),  # æ³¨æ„é€™è£¡æ˜¯ 'author' ä¸”æ˜¯åˆ—è¡¨\n",
    "                    \"published_date\": data.get(\"published_date\", \"\"),\n",
    "                    \"URL\": data.get(\"URL\", \"\"),\n",
    "                    \"metadata\": data.get(\"metadata\", [])\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âŒ JSONæ ¼å¼éŒ¯èª¤ï¼š{filename} â€” {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_json_fields(json_dir):\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(json_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            path = os.path.join(json_dir, filename)\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                result = {\n",
    "                    \"filename\": filename,\n",
    "                    \"titles\": data.get(\"titles\"),\n",
    "                    \"author\": data.get(\"author\"),\n",
    "                    \"published_date\": data.get(\"published_date\"),\n",
    "                    \"URL\": data.get(\"URL\"),\n",
    "                    \"metadata\": data.get(\"metadata\", [])\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âŒ JSONæ ¼å¼éŒ¯èª¤ï¼š{filename} â€” {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "## 6. æ‰¹æ¬¡è½‰æ›æ•´å€‹è³‡æ–™å¤¾å…§ JSON ç‚º RDF\n",
    "\n",
    "def convert_json_directory_to_rdf(json_dir, output_dir=\"output\", one_file=True):\n",
    "    articles = extract_json_fields(json_dir)\n",
    "\n",
    "    intermediate_file = Path(output_dir) / \"_temp_articles.json\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(intermediate_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    convert_json_to_rdf(intermediate_file, output_dir=output_dir, one_file=one_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = \"./docs/output/2_metadata/done\"\n",
    "out_dir = \"./docs/output/6_rdf\"\n",
    "convert_json_directory_to_rdf(json_dir, out_dir, one_file=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === æ­¥é©Ÿ 1: è¼‰å…¥ TTL æª”æ¡ˆ ===\n",
    "g = Graph()\n",
    "ttl_path = \"./docs/output/6_rdf/all_articles.ttl\"  # ğŸ‘‰ é€™è£¡è«‹æ”¹æˆä½ çš„æª”åï¼Œä¾‹å¦‚ \"data/228äº‹ä»¶.ttl\"\n",
    "g.parse(ttl_path, format=\"turtle\")\n",
    "print(f\"Graph has {len(g)} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¸½å…±æœ‰ 8824 ç­†ä¸‰å…ƒçµ„\n",
      "ä¸é‡è¤‡ä¸‰å…ƒçµ„æ•¸é‡ï¼š8824\n"
     ]
    }
   ],
   "source": [
    "# å–å¾—å”¯ä¸€çš„ä¸‰å…ƒçµ„\n",
    "unique_triples = set(g)\n",
    "print(f\"ç¸½å…±æœ‰ {len(g)} ç­†ä¸‰å…ƒçµ„\")\n",
    "print(f\"ä¸é‡è¤‡ä¸‰å…ƒçµ„æ•¸é‡ï¼š{len(unique_triples)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
